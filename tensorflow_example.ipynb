{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple tensorflow 3-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import tensorflow as tf\n",
    "layers = tf.contrib.slim.layers\n",
    "import tensorflow.contrib.slim as slim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting variables and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_classes = 3  # three-class classification\n",
    "learning_rate = 0.001\n",
    "num_epochs = 20\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "\n",
    "Here, we're using mnist as example data. We filter the data here to just select three classes of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_mnist(data_split):\n",
    "    \"\"\"This function extract mnist images and labels, but only keeps thefirst num_classes classes\"\"\"\n",
    "    y = np.argmax(data_split.labels, axis=1)\n",
    "    to_keep = y < num_classes\n",
    "    return data_split.images[to_keep], data_split.labels[to_keep, :num_classes]\n",
    "    \n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Load mnist train, validation and test data\"\"\"\n",
    "    train_x, train_y = load_mnist(mnist.train)\n",
    "    val_x, val_y = load_mnist(mnist.validation)\n",
    "    test_x, test_y = load_mnist(mnist.test)\n",
    "    \n",
    "    return train_x, train_y, val_x, val_y, test_x, test_y\n",
    "    \n",
    "\n",
    "train_x, train_y, val_x, val_y, test_x, test_y = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining tf variables and creating network\n",
    "\n",
    "This is where the network architecture is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Network inputs. Assuming input images are 28x28x1\n",
    "x_in = tf.placeholder(tf.float32, shape=(None, 28, 28, 1))\n",
    "y_in = tf.placeholder(tf.float32, shape=(None, num_classes))\n",
    "phase = tf.placeholder(tf.bool)  # for batch norm - see http://ruishu.io/2016/12/27/batchnorm/\n",
    "\n",
    "# We are using scoping to make the code cleaner - e.g. each activation layer\n",
    "# has a relu, so let's only say that once\n",
    "activation_layers = [layers.conv2d, layers.fully_connected]\n",
    "\n",
    "with tf.contrib.slim.arg_scope(activation_layers, activation_fn=tf.nn.relu):\n",
    "    with tf.contrib.slim.arg_scope([layers.batch_norm], center=True, scale=True, is_training=phase):\n",
    "        \n",
    "        enc = layers.conv2d(x_in, 16, 3)\n",
    "        enc = layers.max_pool2d(enc, 2, 2)\n",
    "        enc = layers.batch_norm(enc)\n",
    "        \n",
    "        enc = layers.conv2d(enc, 32, 3)\n",
    "        enc = layers.max_pool2d(enc, 2, 2)\n",
    "        enc = layers.batch_norm(enc)\n",
    "        \n",
    "        enc = layers.conv2d(enc, 64, 3)\n",
    "        enc = layers.max_pool2d(enc, 2, 2)\n",
    "        enc = layers.batch_norm(enc)\n",
    "        \n",
    "        bsize, h, w, c = enc.shape.as_list()\n",
    "\n",
    "        # dense layers\n",
    "        dense = tf.reshape(enc, [-1, h*w*c])\n",
    "        dense = layers.fully_connected(dense, 128)\n",
    "        dense = layers.batch_norm(dense)\n",
    "        \n",
    "        dense = layers.fully_connected(dense, 128)\n",
    "        output = layers.fully_connected(dense, num_classes, activation_fn=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining losses and updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We let tensorflow do the sigmoid nonlinearity for us\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y_in, logits=output))\n",
    "\n",
    "# This defines the function which will update the network weights\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train_step = slim.learning.create_train_op(loss, optimizer)\n",
    "\n",
    "# Some code to give us the accuracy\n",
    "labels = tf.argmax(y_in, axis=1)\n",
    "predictions = tf.argmax(output, axis=1)\n",
    "accuracy = tf.contrib.metrics.accuracy(labels=labels, predictions=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_feed_dict(x_data, y_data, batch_size):\n",
    "    \"\"\"A helper function to generate minibatches from training\n",
    "    OR validation data\"\"\"\n",
    "\n",
    "    for start_idx in range(0, len(x_data), batch_size):\n",
    "        end_idx = min(len(x_data), start_idx + batch_size)\n",
    "\n",
    "        x = x_data[start_idx:end_idx].reshape(-1, 28, 28, 1)\n",
    "        y = y_data[start_idx:end_idx]\n",
    "\n",
    "        yield {y_in: y, x_in: x}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "-- Training loss: 0.02292\n",
      "-- Training accuracy: 0.99044\n",
      "-- Validation loss: 0.84239\n",
      "-- Validation accuracy: 0.36829\n",
      "Epoch:  1\n",
      "-- Training loss: 0.00442\n",
      "-- Training accuracy: 0.99761\n",
      "-- Validation loss: 2.25883\n",
      "-- Validation accuracy: 0.36829\n",
      "Epoch:  2\n",
      "-- Training loss: 0.00188\n",
      "-- Training accuracy: 0.99907\n",
      "-- Validation loss: 2.90988\n",
      "-- Validation accuracy: 0.36829\n",
      "Epoch:  3\n",
      "-- Training loss: 0.00070\n",
      "-- Training accuracy: 0.99977\n",
      "-- Validation loss: 3.80990\n",
      "-- Validation accuracy: 0.36829\n",
      "Epoch:  4\n",
      "-- Training loss: 0.00011\n",
      "-- Training accuracy: 1.00000\n",
      "-- Validation loss: 4.01506\n",
      "-- Validation accuracy: 0.36829\n",
      "Epoch:  5\n",
      "-- Training loss: 0.00002\n",
      "-- Training accuracy: 1.00000\n",
      "-- Validation loss: 4.12315\n",
      "-- Validation accuracy: 0.36829\n",
      "Epoch:  6\n",
      "-- Training loss: 0.00001\n",
      "-- Training accuracy: 1.00000\n",
      "-- Validation loss: 4.07140\n",
      "-- Validation accuracy: 0.36829\n",
      "Epoch:  7\n",
      "-- Training loss: 0.00001\n",
      "-- Training accuracy: 1.00000\n",
      "-- Validation loss: 3.88432\n",
      "-- Validation accuracy: 0.36829\n",
      "Epoch:  8\n",
      "-- Training loss: 0.00001\n",
      "-- Training accuracy: 1.00000\n",
      "-- Validation loss: 3.58318\n",
      "-- Validation accuracy: 0.36829\n",
      "Epoch:  9\n",
      "-- Training loss: 0.00001\n",
      "-- Training accuracy: 1.00000\n",
      "-- Validation loss: 3.18883\n",
      "-- Validation accuracy: 0.37154\n",
      "Epoch:  10\n",
      "-- Training loss: 0.00000\n",
      "-- Training accuracy: 1.00000\n",
      "-- Validation loss: 2.73554\n",
      "-- Validation accuracy: 0.41622\n",
      "Epoch:  11\n",
      "-- Training loss: 0.00000\n",
      "-- Training accuracy: 1.00000\n",
      "-- Validation loss: 2.25739\n",
      "-- Validation accuracy: 0.52274\n",
      "Epoch:  12\n",
      "-- Training loss: 0.00000\n",
      "-- Training accuracy: 1.00000\n",
      "-- Validation loss: 1.77039\n",
      "-- Validation accuracy: 0.62125\n",
      "Epoch:  13\n",
      "-- Training loss: 0.00000\n",
      "-- Training accuracy: 1.00000\n",
      "-- Validation loss: 1.26501\n",
      "-- Validation accuracy: 0.67607\n",
      "Epoch:  14\n",
      "-- Training loss: 0.00000\n",
      "-- Training accuracy: 1.00000\n",
      "-- Validation loss: 0.78101\n",
      "-- Validation accuracy: 0.74717\n",
      "Epoch:  15\n",
      "-- Training loss: 0.00000\n",
      "-- Training accuracy: 1.00000\n",
      "-- Validation loss: 0.42107\n",
      "-- Validation accuracy: 0.85753\n",
      "Epoch:  16\n",
      "-- Training loss: 0.00000\n",
      "-- Training accuracy: 1.00000\n",
      "-- Validation loss: 0.21425\n",
      "-- Validation accuracy: 0.92603\n",
      "Epoch:  17\n",
      "-- Training loss: 0.00000\n",
      "-- Training accuracy: 1.00000\n",
      "-- Validation loss: 0.10942\n",
      "-- Validation accuracy: 0.96132\n",
      "Epoch:  18\n",
      "-- Training loss: 0.00000\n",
      "-- Training accuracy: 1.00000\n",
      "-- Validation loss: 0.05607\n",
      "-- Validation accuracy: 0.97903\n",
      "Epoch:  19\n",
      "-- Training loss: 0.00000\n",
      "-- Training accuracy: 1.00000\n",
      "-- Validation loss: 0.03141\n",
      "-- Validation accuracy: 0.99017\n",
      "Epoch:  20\n",
      "-- Training loss: 0.00000\n",
      "-- Training accuracy: 1.00000\n",
      "-- Validation loss: 0.02020\n",
      "-- Validation accuracy: 0.99342\n",
      "Epoch:  21\n",
      "-- Training loss: 0.00000\n",
      "-- Training accuracy: 1.00000\n",
      "-- Validation loss: 0.01419\n",
      "-- Validation accuracy: 0.99674\n",
      "Epoch:  22\n",
      "-- Training loss: 0.00000\n",
      "-- Training accuracy: 1.00000\n",
      "-- Validation loss: 0.01112\n",
      "-- Validation accuracy: 0.99740\n",
      "Epoch:  23\n",
      "-- Training loss: 0.00000\n",
      "-- Training accuracy: 1.00000\n",
      "-- Validation loss: 0.00952\n",
      "-- Validation accuracy: 0.99870\n",
      "Epoch:  24\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-12787a136577>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mfeed_dictionary\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mget_feed_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mfeed_dictionary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrn_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrn_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dictionary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mall_train_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrn_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/michael/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 767\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/michael/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 965\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/michael/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1015\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m/home/michael/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1020\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1022\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/michael/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1004\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Do training here... make sure to pass phase=True in the feed_dict for training, phase=False for testing\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    print \"Epoch: \", epoch\n",
    "    \n",
    "    #####################\n",
    "    # TRAINING\n",
    "    \n",
    "    all_train_losses = []\n",
    "    all_train_accuracies = []\n",
    "    \n",
    "    for feed_dictionary in get_feed_dict(train_x, train_y, batch_size):\n",
    "        \n",
    "        feed_dictionary[phase] = True\n",
    "        _, trn_loss, trn_acc = sess.run([train_step, loss, accuracy], feed_dictionary)\n",
    "        \n",
    "        all_train_losses.append(trn_loss)\n",
    "        all_train_accuracies.append(trn_acc)\n",
    "        \n",
    "    print \"-- Training loss: %0.5f\" % np.mean(all_train_losses)\n",
    "    print \"-- Training accuracy: %0.5f\" % np.mean(all_train_accuracies)\n",
    "        \n",
    "    #####################\n",
    "    # VALIDATION\n",
    "    \n",
    "    all_validation_losses = []\n",
    "    all_validation_accuracies = []\n",
    "    \n",
    "    for feed_dictionary in get_feed_dict(val_x, val_y, batch_size):\n",
    "        \n",
    "        feed_dictionary[phase] = False\n",
    "        val_loss, val_acc = sess.run([loss, accuracy], feed_dictionary)        \n",
    "        \n",
    "        all_validation_losses.append(val_loss)\n",
    "        all_validation_accuracies.append(val_acc)\n",
    "                \n",
    "    print \"-- Validation loss: %0.5f\" % np.mean(all_validation_losses)\n",
    "    print \"-- Validation accuracy: %0.5f\" % np.mean(all_validation_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
